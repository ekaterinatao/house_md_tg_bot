{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-d8lGXSe8BQw-YRXBdXL_Au_7L34l6xK",
      "authorship_tag": "ABX9TyOy91z9NlLAcDw5TgHVCuM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaterinatao/house_md_tg_bot/blob/main/generative/NLP_2_HW2_house_md_bot_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Код запускается коррекно только на `GPU`"
      ],
      "metadata": {
        "id": "VSbFNvuciL-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl transformers ftfy gradio accelerate > 0.20.1 git+https://github.com/huggingface/peft.git -Uqqq\n",
        "!pip install bitsandbytes einops datasets wandb -Uqqq\n",
        "!pip install intel-extension-for-transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUFSjBSj3wTC",
        "outputId": "8e9d9276-9ee8-46d2-f6c0-7886bc09a2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm, trange\n",
        "from dataclasses import dataclass\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (AutoTokenizer, AutoModel,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          GenerationConfig)\n",
        "from peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "pKg8gSL8eyHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    seed = 64\n",
        "    dataset = 'ekaterinatao/house_md_context3'\n",
        "    model_id = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
        "    checkpoint = \"ekaterinatao/house-md-tynyLlama\"\n",
        "    batch_size = 8\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    learning_rate = 2e-3\n",
        "    num_epochs = 1\n",
        "    weight_decay = 0.001\n",
        "    gradient_accumulation_steps = 2\n",
        "    optim = \"paged_adamw_32bit\"\n",
        "    max_grad_norm = 0.3\n",
        "    max_steps = 1000\n",
        "    warmup_ratio = 0.03\n",
        "    lr_scheduler_type = \"constant\"\n",
        "    lora_alpha = 32\n",
        "    lora_dropout = 0.05\n",
        "    lora_rank = 32\n",
        "\n",
        "config = TrainingConfig()"
      ],
      "metadata": {
        "id": "MwLHdbl6fVE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = config.seed\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "mwqLBLsFfaKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'device is {config.device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UuDsP2y9fLC",
        "outputId": "be800ff4-22b5-4c3d-aa43-ebdb7b55d6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "BZrgm6u4PMva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config_eval = PeftConfig.from_pretrained(config.checkpoint)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(\n",
        "    peft_config_eval.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.model_id,\n",
        "    add_eos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='left'\n",
        ")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "agRm3RKEPJdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_prompt(question, context=None):\n",
        "    if context is None:\n",
        "        context = ''\n",
        "    else:\n",
        "        context = context\n",
        "    prompt = f\"[INST]\"\n",
        "    prompt += f'Use the given context to guide your about answering the question\\n'\n",
        "    prompt += f\"question: {question}\\n\"\n",
        "    prompt += f\"context: {context}\\n\"\n",
        "    prompt += f\"answer:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_answer(model):\n",
        "    \"\"\"To break generation type 'stop' in input box\"\"\"\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=50,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "        repetition_penalty=2.0,\n",
        "        eos_token_id = tokenizer.eos_token_id,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    question = input(\"Write your question to House MD:\\n\")\n",
        "    context = ''\n",
        "\n",
        "    while True:\n",
        "        prompt = get_test_prompt(question, context=context)\n",
        "        encoding = tokenizer(prompt, return_tensors=\"pt\").to(config.device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        result = text_output.split(\"\\n\")[-1].split('answer:')[-1][2:]\n",
        "        print(f\"House MD: {result}\\n\")\n",
        "        context += f\"\\n{question}\"\n",
        "        question = input(\"You:\\n\")\n",
        "        if \"stop\" in question.lower():\n",
        "            break\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "tnFO5mBNPhZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Пример диалогов с сохранением контекста.  \n",
        "Подбирались гиперпараметры (repetition_penalty, температуры), конструкции промтов. На некоторые вопросы отвечает на других языках (чаще китайский и испанский).  \n",
        "Качество генерации среднее, на парамедицинские темы дает более осмысленные ответы, чем на общие вопросы.  \n",
        "**Вывод:** требуется чистка датасета и добавление диалогов на разные тематики, что на данном этапе намеренно не было реализовано.  "
      ],
      "metadata": {
        "id": "CbLmj-yBg0VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_answer(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yerAnhFSAMgl",
        "outputId": "b0f98e54-1105-469f-bf4d-8043259fdf45"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write your question to House MD:\n",
            "Hi How are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: aqui. I am very happy and excited for this new year! It is my first time in China so it will be an adventure of mine as well but also exciting because there's lots more things that can happen here than just\n",
            "\n",
            "You:\n",
            "I think new year was two months ago. But why are you in China? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: but i don't know how many years it is since last time we met! :) <issue_start><jupyter\\_code>import pandas as pd\rfrom sklearn import preprocessing<p>\rdf =pd['data']\n",
            "\n",
            "You:\n",
            "I do not understand you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: but i don't know how many years it is since last time we met and now that my family has grown up a bit more than before... so maybe this will be an interesting topic for me! :) <issue_start><jup\n",
            "\n",
            "You:\n",
            "If you were House MD what would you advice me against fever and cold?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: I am very sorry for my bad English!   <reponame>john-mccormick/practical_python<filename>.github/.workflows--test/_default157896240\n",
            "\n",
            "You:\n",
            "stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_answer(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF-4gOneeIBO",
        "outputId": "064d7e6f-d1ae-48d9-dfe5-54cc6a04d33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write your question to House MD:\n",
            "If you were House MD what would you advice me against fever and cold?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: a warm bath or hot shower.  2) I'm not sure if this is appropriate for my situation but it seems like an acceptable answer in general since we are talking hypotheticals here (I don’t know how much\n",
            "\n",
            "You:\n",
            "I think hot shower is good idea\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: ot water. It's a very simple thing but it can be done in less than an hour if we have enough money for that kind of things like this! (I don`t know how much time he has)  201\n",
            "\n",
            "You:\n",
            "Are we going to take shower together?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "House MD: I'm not sure. But if it was a very warm day then maybe just stay in bed for an hour or two beforehand so that when they come out of their bathroom there are no bacteria around them (and hopefully also\n",
            "\n",
            "You:\n",
            "stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для тестирования чат-бота с сохранением контекста запустить ячейку ниже\n",
        "Для прекращения генерации напечатать `stop` в stdin"
      ],
      "metadata": {
        "id": "d7gBqRAyk-pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_answer(trained_model)"
      ],
      "metadata": {
        "id": "wmGPVDb6ktuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRADIO DEMO\n",
        "Не удалось реализовать код без использования GPU, поэтому демо с постоянным хостингом сделать не удалось (т.к. это возможно только платно).  \n",
        "  \n",
        "### Тестирование чат-бота\n",
        "* При запуске следующей ячейки будет реализована временная демо версия в `gradio` для тестирования чат-бота.\n",
        "* Тест с сохранением контекста можно провести в ноутбуке, запустив ячейку выше с функцией `generate_answer`"
      ],
      "metadata": {
        "id": "BrmQDgoIgaFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(\n",
        "    question,\n",
        "    model=trained_model\n",
        "):\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=50,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "        repetition_penalty=2.0,\n",
        "        eos_token_id = tokenizer.eos_token_id,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    prompt = get_test_prompt(question)\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(config.device)\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    result = text_output.split(\"\\n\")[-1].split('answer:')[-1][2:]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "title = \"HouseMD bot\"\n",
        "description = \"Gradio Demo for bot. To use it, simply add your text message.\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=get_answer,\n",
        "    inputs=gr.Textbox(label=\"Input message to House MD\", lines=2),\n",
        "    outputs=gr.Textbox(label=\"House MD's answer\"),\n",
        "    title=title,\n",
        "    description=description\n",
        ")\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "nBbSvWjyJiiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b59be81-25cc-4b11-9c9b-7e30da7f8131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on public URL: https://05ea13b9a9def4763d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05ea13b9a9def4763d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://05ea13b9a9def4763d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZkA8t1dVC1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}